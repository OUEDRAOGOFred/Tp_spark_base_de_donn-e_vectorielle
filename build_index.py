"""
Script de vectorisation et indexation FAISS
√âtape 2 : G√©n√©ration des embeddings et cr√©ation de l'index
"""

import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import os
import pickle
import time

# Configuration
CORPUS_FILE = "docs_medical.csv"
EMBEDDINGS_FILE = "embeddings_medical.npy"
INDEX_FILE = "medical_faiss.index"
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
BATCH_SIZE = 32
USE_GPU = False  # Mettre √† True si GPU disponible

# Configuration FAISS
USE_IVF = True  # Utiliser IndexIVFPQ pour de meilleures performances
N_CLUSTERS = 100  # Nombre de clusters pour IVF
N_PROBE = 10  # Nombre de clusters √† explorer lors de la recherche

def load_corpus():
    """
    Charger le corpus pr√©par√©
    """
    print(f"üìÅ Chargement du corpus depuis {CORPUS_FILE}...")
    
    if not os.path.exists(CORPUS_FILE):
        raise FileNotFoundError(
            f"Le fichier {CORPUS_FILE} n'existe pas. "
            "Ex√©cutez d'abord prepare_corpus.py"
        )
    
    df = pd.read_csv(CORPUS_FILE)
    print(f"‚úÖ Corpus charg√©: {len(df)} documents")
    
    return df

def load_model():
    """
    Charger le mod√®le Sentence-Transformer
    """
    print(f"\nü§ñ Chargement du mod√®le {MODEL_NAME}...")
    
    # D√©sactiver les warnings
    import warnings
    warnings.filterwarnings('ignore')
    
    # Configurer l'environnement
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
    os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
    
    model = SentenceTransformer(MODEL_NAME)
    
    # Mettre sur GPU si disponible et demand√©
    if USE_GPU:
        import torch
        if torch.cuda.is_available():
            model = model.to('cuda')
            print("‚úÖ Mod√®le charg√© sur GPU")
        else:
            print("‚ö†Ô∏è  GPU non disponible, utilisation CPU")
    else:
        print("‚úÖ Mod√®le charg√© sur CPU")
    
    return model

def generate_embeddings(df, model):
    """
    G√©n√©rer les embeddings pour tout le corpus
    """
    print(f"\nüîÑ G√©n√©ration des embeddings...")
    print(f"   Taille du batch: {BATCH_SIZE}")
    
    # Pr√©parer les textes (Question + Answer)
    texts = []
    for _, row in df.iterrows():
        text = f"{row['Question']} {row['Answer']}"
        texts.append(text)
    
    # G√©n√©rer les embeddings par batch avec barre de progression
    embeddings = []
    
    start_time = time.time()
    
    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc="G√©n√©ration"):
        batch = texts[i:i + BATCH_SIZE]
        batch_embeddings = model.encode(
            batch,
            normalize_embeddings=True,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        embeddings.append(batch_embeddings)
    
    # Combiner tous les embeddings
    embeddings = np.vstack(embeddings)
    
    elapsed = time.time() - start_time
    print(f"‚úÖ Embeddings g√©n√©r√©s: {embeddings.shape}")
    print(f"   Temps: {elapsed:.2f}s ({len(texts)/elapsed:.1f} docs/sec)")
    print(f"   Dimension: {embeddings.shape[1]}")
    
    return embeddings

def save_embeddings(embeddings):
    """
    Sauvegarder les embeddings
    """
    print(f"\nüíæ Sauvegarde des embeddings dans {EMBEDDINGS_FILE}...")
    np.save(EMBEDDINGS_FILE, embeddings)
    print(f"‚úÖ Embeddings sauvegard√©s ({embeddings.nbytes / 1024 / 1024:.2f} MB)")

def create_faiss_index(embeddings):
    """
    Cr√©er l'index FAISS
    """
    print(f"\nüî® Cr√©ation de l'index FAISS...")
    
    dimension = embeddings.shape[0]
    n_vectors = embeddings.shape[0]
    
    print(f"   Nombre de vecteurs: {n_vectors}")
    print(f"   Dimension: {dimension}")
    
    if USE_IVF and n_vectors > N_CLUSTERS * 39:
        # IndexIVFPQ pour de meilleures performances
        print(f"   Type: IndexIVFPQ (clusters={N_CLUSTERS})")
        
        # Quantizer de base
        quantizer = faiss.IndexFlatIP(embeddings.shape[1])
        
        # Index IVF avec Product Quantization
        # M = nombre de sous-vecteurs (doit diviser la dimension)
        # nbits = nombre de bits par sous-vecteur
        m = 8  # Ajuster selon la dimension
        nbits = 8
        
        index = faiss.IndexIVFPQ(
            quantizer,
            embeddings.shape[1],
            N_CLUSTERS,
            m,
            nbits
        )
        
        # Entra√Æner l'index
        print(f"   Entra√Ænement de l'index...")
        index.train(embeddings.astype('float32'))
        index.nprobe = N_PROBE
        
        print(f"   Ajout des vecteurs...")
        index.add(embeddings.astype('float32'))
        
        print(f"‚úÖ Index IVF cr√©√© avec succ√®s")
        
    else:
        # IndexFlatIP pour les petits corpus ou si IVF d√©sactiv√©
        print(f"   Type: IndexFlatIP (recherche exacte)")
        
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings.astype('float32'))
        
        print(f"‚úÖ Index Flat cr√©√© avec succ√®s")
    
    print(f"   Vecteurs index√©s: {index.ntotal}")
    
    return index

def save_index(index):
    """
    Sauvegarder l'index FAISS
    """
    print(f"\nüíæ Sauvegarde de l'index dans {INDEX_FILE}...")
    faiss.write_index(index, INDEX_FILE)
    
    # Taille du fichier
    file_size = os.path.getsize(INDEX_FILE) / 1024 / 1024
    print(f"‚úÖ Index sauvegard√© ({file_size:.2f} MB)")

def test_index(index, embeddings, df):
    """
    Tester l'index avec quelques requ√™tes
    """
    print(f"\nüß™ Test de l'index...")
    
    # Prendre quelques exemples al√©atoires
    test_indices = np.random.choice(len(df), 3, replace=False)
    
    for idx in test_indices:
        query_vec = embeddings[idx:idx+1].astype('float32')
        
        # Recherche
        scores, indices = index.search(query_vec, k=5)
        
        print(f"\nüìù Test avec document {idx}:")
        print(f"   Question: {df.iloc[idx]['Question'][:80]}...")
        print(f"   Top 5 r√©sultats:")
        
        for rank, (score, result_idx) in enumerate(zip(scores[0], indices[0]), 1):
            print(f"      {rank}. [Score: {score:.4f}] {df.iloc[result_idx]['Question'][:60]}...")

def create_metadata():
    """
    Cr√©er un fichier de m√©tadonn√©es pour l'index
    """
    metadata = {
        'model_name': MODEL_NAME,
        'corpus_file': CORPUS_FILE,
        'embeddings_file': EMBEDDINGS_FILE,
        'index_file': INDEX_FILE,
        'dimension': None,  # Sera rempli plus tard
        'n_vectors': None,
        'index_type': 'IVF' if USE_IVF else 'Flat',
        'created_at': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    return metadata

def main():
    """
    Pipeline complet de vectorisation et indexation
    """
    print("=" * 60)
    print("üîÑ VECTORISATION ET INDEXATION FAISS")
    print("=" * 60)
    
    # √âtape 1: Charger le corpus
    df = load_corpus()
    
    # √âtape 2: Charger le mod√®le
    model = load_model()
    
    # √âtape 3: G√©n√©rer les embeddings
    embeddings = generate_embeddings(df, model)
    
    # √âtape 4: Sauvegarder les embeddings
    save_embeddings(embeddings)
    
    # √âtape 5: Cr√©er l'index FAISS
    index = create_faiss_index(embeddings)
    
    # √âtape 6: Sauvegarder l'index
    save_index(index)
    
    # √âtape 7: Tester l'index
    test_index(index, embeddings, df)
    
    # √âtape 8: Cr√©er les m√©tadonn√©es
    metadata = create_metadata()
    metadata['dimension'] = embeddings.shape[1]
    metadata['n_vectors'] = embeddings.shape[0]
    
    with open('index_metadata.pkl', 'wb') as f:
        pickle.dump(metadata, f)
    
    print("\n" + "=" * 60)
    print("‚úÖ INDEXATION TERMIN√âE AVEC SUCC√àS !")
    print("=" * 60)
    print(f"\nüìä R√©sum√©:")
    print(f"   - Documents index√©s: {metadata['n_vectors']}")
    print(f"   - Dimension des embeddings: {metadata['dimension']}")
    print(f"   - Type d'index: {metadata['index_type']}")
    print(f"   - Fichiers cr√©√©s:")
    print(f"      ‚Ä¢ {EMBEDDINGS_FILE}")
    print(f"      ‚Ä¢ {INDEX_FILE}")
    print(f"      ‚Ä¢ index_metadata.pkl")
    
    return index, embeddings, metadata

if __name__ == "__main__":
    main()
